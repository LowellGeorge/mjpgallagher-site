<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI 101 — BOOKS & Links</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <h1><a href="index.html">BOOKS & Links</a></h1>
        <nav>
            <a href="index.html">ABOUT</a>
            <a href="reading.html">READING</a>
            <a href="bookshelf.html">BOOKSHELF</a>
            <a href="lists-sources.html">Lists / Sources</a>
            <a href="ai-101.html" class="active">AI 101</a>
        </nav>
    </header>

    <main>
        <h2>Generative Models: What They Are, How They Work and Where They Came From</h2>

        <h3>Core Learning Resources</h3>
        <ol>
            <li><a href="https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">3Blue1Brown Neural Networks Series</a> – YouTube video series on neural networks</li>
            <li><a href="https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">3Blue1Brown Linear Algebra Series</a> – Linear algebra foundation series</li>
            <li><a href="https://arstechnica.com/science/2023/07/a-jargon-free-explanation-of-how-ai-large-language-models-work/">Ars Technica: Jargon-Free LLM Explanation</a> – How AI large language models work</li>
            <li><a href="https://arxiv.org/pdf/2501.09223">Foundations of Large Language Models</a> – arXiv research paper</li>
            <li><a href="https://www.youtube.com/watch?v=i8D90DkCLhI">Welch Labs: Visual AI Introduction</a></li>
            <li><a href="https://www.youtube.com/watch?v=0VLAoVGf_74">Welch Labs: DeepSeek Transformer Analysis</a></li>
            <li><a href="https://learn.deeplearning.ai/courses/how-transformer-llms-work/lesson/nfshb/introduction">DeepLearning.AI: How Transformer LLMs Work</a> – Free course (registration required)</li>
            <li><a href="https://distill.pub/">Distill Series on Pre-ChatGPT Evolution</a> – Visual summaries of AI evolution</li>
            <li><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> – Jay Alammar</li>
            <li><a href="https://www.youtube.com/watch?v=UPkwqG0DfGQ">Transformer Explanation Video</a> – Using Jay Alammar's explainer</li>
            <li><a href="https://www.youtube.com/watch?v=HA_jGZdYvHI">Notebook LM "Podcast" on Attention Paper</a> – AI-generated podcast on "Attention is All You Need"</li>
            <li><a href="https://www.youtube.com/watch?v=UZDiGooFs54&list=LL&index=2">The Moment We Stopped Understanding AI (AlexNet)</a></li>
            <li><a href="https://www.youtube.com/watch?v=zjkBMFhNj_g">Andrej Karpathy: The Busy Person's Intro to LLMs</a></li>
            <li><a href="https://www.amazon.co.uk/Why-Machines-Learn-Elegant-Behind/dp/B0CLVQBC3B/">Why Machines Learn: The Elegant Maths Behind Modern AI</a> – Anil Ananthaswamy (Book)</li>
            <li><a href="https://cset.georgetown.edu/article/the-surprising-power-of-next-word-prediction-large-language-models-explained-part-1/">Helen Toner's LLM Explainer</a> – "Next word prediction" in LLMs</li>
            <li><a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html">Anthropic Interpretability Research</a></li>
            <li><a href="https://simonwillison.net/2024/Mar/8/gpt-4-barrier/">Simon Willison: Model Capability Assessment</a></li>
            <li><a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">The Bitter Lesson</a> – Scaling and compute in AI</li>
            <li><a href="https://gwern.net/scaling-hypothesis">Gwern on Scaling</a></li>
        </ol>

        <h3>Resources for a Deeper Dive</h3>
        <ol>
            <li><a href="http://neuralnetworksanddeeplearning.com/">Michael Nielsen's Neural Networks eBook</a> – Free online ebook (recommended by 3Blue1Brown)</li>
            <li><a href="https://www.youtube.com/playlist?list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC">Andrej Karpathy's CS231n Course</a> – Stanford University, Winter 2016 (start with lecture 2)</li>
            <li><a href="https://www.deeplearningbook.org/">Deep Learning Textbook</a> – Free online version</li>
            <li><a href="http://deeplearning.stanford.edu/tutorial/">Stanford Deep Learning Tutorial</a></li>
            <li><a href="https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE">Ilya Sutskever's AI Reading List</a> – For John Carmack</li>
        </ol>

        <h3>How to Use Them?</h3>
        <ol>
            <li><a href="https://www.youtube.com/watch?v=EWvNQjAaOHw">Andrej Karpathy's LLM Use Cases Guide</a></li>
            <li><a href="https://open.substack.com/pub/peteryang/p/an-opinionated-guide-on-which-ai-model-2025">March 2025 Model Capabilities Review</a> – "Opinionated guide on which AI model"</li>
            <li><a href="https://substack.com/@strangeloopcanon/p-158423604">Gemini Quality Assessment</a> – "Why Gemini is actually very good"</li>
            <li><a href="https://marketurbanism.com/2025/03/07/deep-research-supermajority-laws-around-the-states/">Land Use Research with Deep Research</a> – Successful practical application example</li>
            <li><a href="https://x.com/cblatts/status/1897377915622642173">Economics Research with Deep Research</a> – Economist's successful use case</li>
            <li><a href="https://x.com/david_perell/status/1896759236081639569">Tyler Cowen's Model Usage</a> – How different models are used</li>
        </ol>

        <h3>What Are the Dangers of Using Them?</h3>
        <ol>
            <li><a href="https://arxiv.org/pdf/2506.08872">MIT Cognitive Debt Study</a> – "Better work product, but you become less smart because you didn't do the work"</li>
        </ol>

        <h3>Personal Favorite Use Cases</h3>
        <p><strong>Parallel Translation Reading:</strong> Using Claude on mobile for non-English texts. Live translations often superior to published versions. Side-by-side paragraph reading.</p>
    </main>

    <footer>
        <p>&copy; 2024 Michael Gallagher. Contact: <a href="mailto:michaeljpgallagher@gmail.com">michaeljpgallagher@gmail.com</a></p>
    </footer>
</body>
</html>
